\section{Experiments} 
\label{sec:experiments}

To evaluate the effect of our changes on the efficiency of the planner
on practical inputs, we ran a series of experiments using the XTAG
Grammar \cite{xtag01:_tr}, a large-scale tree-adjoining grammar of
English, from which we selected all lexicon entries for seven words we
used in the experiments. We set up inputs to generate sentences of the
form

$$S_1 \;\mathrm{and}\; S_2 \;\mathrm{and}\; \ldots \;\mathrm{and}\;
S_n,$$

i.e.\ a conjunction of $n$ sentences. Each sentence $S_i$ is of the
form ``the X admires the Y'' or ``the X gives the Y the Z''; that is,
we vary the arity $k$ of the verb to be 2 or 3. For X, Y, and Z are
all of a form like ``the rich businessman'', i.e.\ like the rabbit in
Fig.~\ref{fig:white-rabbit-as-planning}, the individual must be
described with one noun and one adjective to distinguish it from all
other individuals. We translate these generation problems into
planning problems with the new encoding as shown in
Fig.~\ref{fig:white-rabbit-as-planning}.

\begin{figure}
  \centering
  la la la
  \caption{Runtimes for $k=2$.}
  \label{fig:runtimes-k2}
\end{figure}

The results for $k=2$ are shown in Fig.~\ref{fig:runtimes-k2}; the
curves end where the planner exceeded its five-minute timeout.  As the
graph shows, the improved preprocessor has a huge impact on the
runtimes: \todo{refer to data}.  We can also see that best-first
search with the helpful actions heuristic is the best-performing
search algorithm.  While the EHC search is still faster than
best-first search without helpful actions, this says less about the
quality of the search algorithm itself than it does about the
usefulness of the helpful actions heuristic.

The improvements we have made in this paper bring the overall planner
runtimes into a range where they can be useful in practical natural
language generation applications.  For instance, the 24-word sentence
for $k=2$ and $n=3$ is generated in under four seconds; before our
changes, FF was not able to generate this sentence in less than
\todo{XXX time}.  Nevertheless, our experiment also shows that the
current technology does not yet scale to larger instances.  It should
be noted that although we used all lexicon entries from XTAG for the
words in our sentences, our experiment grammar still only contained
lexicon entries for seven different words.  It would not be
unreasonable to assume a grammar with a larger vocabulary, which would
then be difficult for the current planner to handle.

One final aspect of our experiments concerns the difference between
the new encoding of generation problems into planning problems
proposed above and the old encoding mentioned in Section \todo{XXX}.
We ran a variant of the above experiment where X, Y, and Z could be
realized with a single noun (``the businessman'') and did not require
an adjective to be uniquely identifiable. On this data, search on the
new encoding tended to be mildly faster for small to medium-sized
instances than on the old encoding (1.6 sec vs.\ 2.7 sec for EHC
search on $k=2,n=4$), whereas preprocessing was considerably faster
for the new encoding even after our improvements (0.7 sec vs.\ 2.7 sec
on $k=2,n=4$).  However, for larger instances the new encoding
exceeded the five-minute timeout earlier than the old encoding did.
Overall, the picture for the old and new encoding was roughly
comparable, with one notable exception: Under the old encoding, EHC
search was slightly faster than best-first search with helpful
actions (7.2 sec vs.\ 10.9 sec at $k=2,n=5$).  This is an artefact of
the fact that because the input specified the sentence word for word,
the relaxed plan heuristic did not underestimate the plan length as
grossly as in other cases.  As we have seen, adding even a single
distractor for every noun phrase already negates this advantage.



%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "main"
%%% End: 

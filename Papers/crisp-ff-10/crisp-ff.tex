




\section{FF Pitfalls and Fixes}
\label{sec:crisp-ff}




It turns out that, as far as the suitability of commonly used
pre-processing and search techniques are concerned, sentence
generation diverges significantly from most known benchmark
domains. We examine this here, in detail, for the case of FF
\cite{HoffmannNebel01}. We show how certain hitherto irrelevant
details of the pre-processing suddenly become deadly obstacles to
performance, and how certain widely used search techniques -- namely
goal orderings and the relaxed plan heuristic -- are completely off
the mark and partly even detrimental. For some of the observed
pitfalls, we are able to provide simple fixes; others pose serious
challenges to planning research. We focus first on the pre-processing,
then on the search techniques.













\subsection{Pre-Processing}
\label{sec:crisp-ff:preprocess}



%../ff -o xtag-3-2-domain.lisp -f xtag-3-2-problem.lisp -T -H: 0.10
%../ff -o xtag-3-2-domain.lisp -f xtag-3-2-problem.lisp -T: 1.54
%../ff -o xtag-3-2-domain.lisp -f xtag-3-2-problem.lisp -H: 0.30
%../ff -o xtag-3-2-domain.lisp -f xtag-3-2-problem.lisp: 1.80


%../ff -o xtag-5-2-domain.lisp -f xtag-5-2-problem.lisp -T -H: 0.29
%../ff -o xtag-5-2-domain.lisp -f xtag-5-2-problem.lisp -T: seg fault ie too much memory
%../ff -o xtag-5-2-domain.lisp -f xtag-5-2-problem.lisp -H: 11.08
%../ff -o xtag-5-2-domain.lisp -f xtag-5-2-problem.lisp: seg fault ie too much memory


%../ff -o xtag-3-3-domain.lisp -f xtag-3-3-problem.lisp -T -H: 
%../ff -o xtag-3-3-domain.lisp -f xtag-3-3-problem.lisp -T: seg fault ie too much memory
%../ff -o xtag-3-3-domain.lisp -f xtag-3-3-problem.lisp -H: 
%../ff -o xtag-3-3-domain.lisp -f xtag-3-3-problem.lisp: seg fault ie too much memory








FF's pre-process contains two details that, for a long time until they
were detected, were so detrimental in our domain that we were not able
to seriously propose FF as a practical solution. Simple fixes to these
details are so effective that FF is now a practical (if not perfect)
possibility.



First, while pre-processing the ADL formulas, FF has a sub-routine
that checks the instantiated formulas -- quantifiers already compiled
into grounded conjunctions and disjunctions -- for tautologies. This
involves a loop checking all pairs of sub-formulas within every
conjunction and disjunction, testing whether they are
identical/identical modulo their sign. For example, if we find $\phi$
and $\neg \phi$ within a disjunction, then the disjunction is replaced
by TRUE. Now, we haven't tested thoroughly whether this sub-routine
has any notable effect on the formulas in other benchmarks. It
certainly has no such effect in our domain. The effect it {\em does}
have is take a huge amount of runtime on the long goal conjunctions
stating that we do not wish to have any open substitution nodes nor
any distractors. The simple fix is hence to switch this sub-routine
off. In smaller examples, this gives advantages like $0.29s$
vs.\ $11s$ pre-processing time; in a larger example we ran, original
FF's pre-process doesn't finish within 15 minutes whereas our fix
yields a pre-processing time of about $3s$.


Second, FF's pre-process contains a very small imprudence that
apparently didn't surface in other domains, but that does in ours. FF
distinguishes operators into ``easy'' (DNF precondition) and ``hard''
(non-DNF precondition). The easy ones are instantiated in a
spezialized very effectively implemented procedure, while the hard
ones go through a much less effectively implemented more general
procedure. In particular, that latter procedure uses a data structure
encompassing an integer for every possible instantiation of all
predicates. Since this does not take into account reachability, it is
prohibitively huge in our domain. Now, there actually are no hard
operators in our domain. But FF builds the data structure anyway, even
though it is not needed. In small examples, this results in a couple
of seconds runtime overhead. In larger examples, it exhausts memory
and causes FF to terminate with a segmentation fault.




















\subsection{Search Techniques}
\label{sec:crisp-ff:preprocess:heuristics}





FF's pre-processing pitfalls -- and the fact that they didn't occur to
anyone before -- are certainly baffling, although easily fixed. We now
turn our attention to some of FF's search techniques, variations of
which are used in many planners and which are very much not easily
fixed.


%\item Goal agenda: For some reason that I don't understand in detail,
%  the goal agenda heuristic chooses that the ``subst'' goal must be
%  reached before even the ``expressed'' goal. This leads the planner
%  to generating some arbitrary sentence. Because the example grammar
%  is so small, this leads to a state from which the ``expressed'' goal
%  can no longer be achieved. (A realistic big grammar should be able
%  to recover from this by introducing a conjunction or relative clause
%  or some such -- but of course, this would still lead to a stupid
%  sentence.) FIX: turn goal agenda off. alternatively, develop
%  appropriate goal ordering techniques.


First, we consider Koehler and Hoffmann's
\shortcite{koehler:hoffmann:jair-00} ``reasonable goal orderings'',
variants and extensions of which are used in several planning systems
today, e.g.\ \cite{hoffmann:etal:jair-04,richter:etal:aaai-08}. FF as
used in the 2000 competition implements the ideas from
\cite{koehler:hoffmann:jair-00}, approximating reasonable orders as a
pre-process and then partitioning the goal set into a {\em goal
  agenda}, a sequence of goal subsets. Each subset is posed to the
planner in isolation, ignoring all other goals. The final plan is the
concatenation of the plans for all entries.

%EXAMPLE missing

While the goal agenda and related techniques have proved quite useful
in many domains, in the sentence generation domain they are completely
off-target. Reasonable orders exist {\bf \dots explain using the
  example dedicated to this point how we get to have only the
  ``subst'' goal in the first entry \dots} When achieving this first
goal agenda entry, the planner decides to generate {\em any}
sentence. If it happens to be the wrong sentence, the planner is stuck
in a dead-end. While that dead end is recognized by the relaxed plan
heuristic -- i.e., it is easy to see that the actions needed for
achieving the goal cannot be applied anymore -- certainly this is an
entirely unintended (and a bit amusing) outcome of this
technique. Switching it off, we {\bf \dots example before/after.}

{\bf \dots pose as a challenge \dots if sensible based on example
  observations, speculate a bit how/whether goal ordering techniques
  could be modified to work}









%\item Subst and distractor atoms have a weird lifecycle: They start as
%  false, then get made true at some point, and then false again.
%  After they have been made false, they can never become true
%  again. FIX: no fix just yet. highly non-trivial challenge to
%  generation of h fns. {\bf (QUESTION: does lama do better? do any of
%    the known admissible heuristics do better?)}


While the goal agenda meltdown may be more amusing than alarming, we
next report that relaxed plan heuristics are not far from such a
meltdown either. They are fatally confused by the weird lifecycle of
facts encoding substitution nodes and distractors. In instances where
these constructs are important for the construction of a valid
sentence, these facts are required to be false in the goal, and most
of them are initially false. However, any valid plan must make many of
them true at some intermediate time point (e.g.\ distractors arise
depending on the decisions taken by the planner). Hence, from the
perspective of the relaxed plan heuristic, from one state to the next
whole classes of goals suddenly appear ``out of the blue''. These
goals are not accounted for in the preceding state, and hence the
relaxed plan is completely un-informative.


To give just one concrete example, say we are in the initial state,
and have not yet committed to a sentence. For certain kinds of
sentences ({\bf more concrete here???}), the relaxed plan has length
$1$ because the we only need to choose the desired meaning. However,
in the state $s$ resulting from this action, the mechanisms of the
planning task introduce all the sub-goals that are required in order
to pin-point that meaning (in particular we get a new goal for all the
distractors that must be removed). Hence the relaxed plan is much
longer than before -- arbitrarily longer, in general. For FF's
particular search algorithm, enforced hill-climbing (EHC), this means
that {\em FF performs a blind breadth-first search}: EHC starts at $h$
value $1$ and thereafter needs to solve the whole problem in order to
find a (goal) state with lower $h$ value.

%../ff -o modifiers-10-10-domain.lisp -f modifiers-10-10-problem.lisp -T -H

%\item As a consequence, Enforced hillclimbing: Initial state of
%  example problem has a relaxed graph of length one because
%  ``expressed'' and ``subst'' goals are achieved, and newly introduced
%  violations of ``subst'' and ``distractor'' are not noticed. As a
%  consequence, EHC on the second state spends all of its time in
%  breadth-first search. FIX: don't use EHC.


More generally, from this example we immediately see that, in the
terms of Hoffmann \shortcite{hoffmann:jair-05}, the exit distance from
local minima under $h^+$ is unbounded. Also, we have unrecognized
dead-ends, i.e., dead-end states that have a relaxed plan. An example
is the initial state of a task that features distractors for which
there is no means of removal -- ignoring the distractors, the initial
$h$ value will still be $1$. So our domain is in the most difficult
class of Hoffmann's \shortcite{hoffmann:jair-05} ``planning domain
taxonomy''. While it shares this property with several other
challenging domains, the extremity of turning EHC into a breadth-first
search is unheard of.



{\bf \dots pose as a challenge \dots if sensible based on example
  observations, speculate a bit how/whether heuristics could be
  modified to work \dots a question that we should ideally at least
  partially answer in here is whether other existing heuristics
  actually work better.. we could try at least a little bit with PDBs,
  merge\&shrink, etc}



The only FF technique that appears to work reasonably well is its
action pruning technique, the ``helpful actions''. While the relaxed
plan is often much too short, it appears to always contain at least
one of the actions that actually are useful, and it certainly prunes
the set of avaibale actions quite effectively. For example, in one of
the instances where EHC turns into a breadth-first search, $16875$
nodes are explored without helpful actions; with helpful actions, this
reduces to $3078$. For larger examples, the factor between the two
numbers tends to become larger. {\bf use here larger version of
  modifiers-10-10???}



Since EHC is often completely inadequate, and helpful actions are
quite effective, our FF configuration directly runs best-first search
with helpful actions (as opposed to original FF which switches to
best-first with{\em out} helpful actions if EHC fails).


%\item The only thing that works well is helpful actions. However,
%  these didn't get around to be used due to no helpful actions in
%  best-first search: After failure of EHC, FF falls back to best-first
%  search, but doesn't use the helpful actions heuristic in this
%  search. FIX: simply go straight to BFS with helpful actions.








%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "main"
%%% End: 

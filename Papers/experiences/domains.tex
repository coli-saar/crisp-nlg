\section{Planning in NLG} \label{sec:domains}

In this section, we present our planning domains: the sentence
generation domain and the instruction giving domain.


\subsection{Sentence generation as planning}

Sentence generation is the problem of computing, from a grammar and a
semantic representation, a single sentence that expresses this piece
of meaning.  This problem is traditionally split into several steps
\cite{reiter00building}.  In a first step, called \emph{sentence
  planning}, the semantic representation is first enriched with more
information; for instance, \emph{referring expressions}, which refer
to individuals that we want to talk about, are determined at this
point.  In a second step, called \emph{surface realization}, this
enriched representation is then translated into a natural-language
sentence, using the grammar.

However, determining referring expressions (REs) and realization
interact, so it would be beneficial to perform both steps together.
This was the goal of the SPUD system \cite{Stone2003a}, which
performed both steps together using a top-down generation algorithm
based on tree-adjoining grammars \cite{joshi;etal1997} whose lexical
entries were equipped with semantic and pragmatic information.
Unfortunately, SPUD suffered from having to explore a huge search
space, and had to resort to a non-optimal greedy search strategy to
retain reasonable efficiency.

\cite{KolSto07} attempt to improve the efficiency by translating the
SPUD sentence generation problem into a planning problem and using a
planning algorithm for generation.  We will now illustrate how they do
this by a (simplified) example.

\begin{figure}
  \centering
  \includegraphics[width=\columnwidth]{pic-grammar}
  \caption{The example grammar.}
  \label{fig:white-rabbit-sleeps-grammar}
\end{figure}

\begin{figure}
  \centering
  \includegraphics[width=\columnwidth]{pic-derivation}
  \caption{Derivation of ``The white rabbit sleeps.''}
  \label{fig:white-rabbit-sleeps-deriv}
\end{figure}

Consider a knowledge base containing the individuals $r_1$ and $r_2$;
$r_1$ and $r_2$ are rabbits, $r_1$ is white and $r_2$ is brown, and
$r_1$ sleeps.  Let's say we want to express the information
$\{\mathsf{sleep}(r_1)\}$ using the tree-adjoining grammar shown in
Fig.~\ref{fig:white-rabbit-sleeps-grammar}.  This grammar consists of
\emph{elementary trees}, each of which contributes certain
\emph{semantic content}.  We can instantiate these trees by
substituting individuals for \emph{semantic roles}, such as
$\mathsf{self}$ and $\mathsf{subj}$, and then combine the tree
instances as shown in Fig.~\ref{fig:white-rabbit-sleeps-deriv} to
obtain the sentence ``The white rabbit sleeps''.  The SPUD algorithm
computes this grammatical derivation by starting with the elementary
tree for ``sleeps''; this satisfies the need to convey the semantic
information, but introduces a need to generate a noun phrase (NP) for
the subject, which also refers uniquely to the target referent $r_1$.
In a second step, it substitutes the tree for ``the rabbit'' into the
open NP leaf, which makes the derivation grammatically complete; but
as there are two different individuals that could be described as
``the rabbit'' -- technically, $r_2$ is still a \emph{distractor} for
the referring expression --, we are still not finished.  This is why
we add the tree for ``white'' to the derivation by the
\emph{adjunction} operation.  This makes the derivation syntactically
and semantically complete, and we are done.

\begin{figure}
  \centering
  xxxx
  \caption{Generating ``The white rabbit sleeps'' as a planning problem.}
  \label{fig:white-rabbit-as-planning}
\end{figure}

This process has clear parallels to planning: We manipulate a state by
applying actions in order to achieve a goal.  We can make this
connection precise by translating the SPUD problem into a planning
problem; the relevant excerpt of this problem is shown in
Fig.~\ref{fig:white-rabbit-as-planning}.  Each action in this planning
problem corresponds to the addition of a single elementary tree to the
derivation; the first parameter of the action is a node name in the
derivation tree, and the further parameters stand for the individuals
to which the semantic roles will be instantiated.  The syntactic
preconditions and effects are encoded using $\mathsf{open}$ and
$\mathsf{canadjoin}$ literals; the status of each referring expression
is tracked using $\mathsf{distractor}$ literals.  Notice that the
action effects contain terms of the form $\mathsf{subj}(u)$, which
construct new node names.  In order to represent the planning problem
in PDDL, these terms can be eliminated by estimating an upper bound
$n$ for the plan length, making $n$ copies of each action, ensuring
that copy $i$ can only be applied in step $i$, and replacing the term
$\mathsf{subj}(u)$ in an action copy by the constant
$\mathsf{subj}_i$.

In the example, the following plan solves the planning problem:
\begin{enumerate}
\item $\mathsf{sleeps}(\mathsf{root}, r_1)$
\item $\mathsf{rabbit}(\mathsf{subj}(\mathsf{root}), r_1)$
\item $\mathsf{white}(\mathsf{subj}(\mathsf{root}), r_1)$
\end{enumerate}

The grammatical derivation in
Fig.~\ref{fig:white-rabbit-sleeps-deriv}, and therefore the generated
sentence, can be systematically reconstructed from this plan.
Therefore we can solve the sentence generation problem via the detour
through planning and bring current search heuristics for planning to
bear on generation.








\subsection{Planning in instruction giving}

The object of the GIVE Challenge (``Generating Instructions in Virtual
Environments''; Koller et al.\
\citeyear{alexander07:_shared_task_propos}) is to build an NLG system
which is able to give natural-language instructions which will guide a
human user in performing some task in a virtual environment.  From an
NLG perspective, GIVE makes for an interesting challenge because it is
a theory-neutral task that exercises all components of an NLG system,
and emphasizes the study of communication in a (simulated) physical
environment.  It also has the advantage that the user and the NLG
system can be physically in different places, as long as the 3D client
and the NLG system are connected over a network.  This makes it
possible to evaluate GIVE NLG systems on a large scale over the
Internet.  The first GIVE evaluation will take place in late 2008;
currently eight research teams from five countries are working on
developing systems to participate in the challenge.

\begin{figure}
\centering
\includegraphics[width=1 \columnwidth]{give_world_no_expl}
\caption{Map of the GIVE development world.}
  \label{fig:give-development-world}
\end{figure}

A map of an example world is shown in
Fig.~\ref{fig:give-development-world}.  In this world, the user's task
is to pick up a trophy in the top left room.  The trophy is hidden in
a safe behind a picture, and the user must therefore first move the
picture away and open the safe by pushing a sequence of buttons (the
small square boxes on the walls) in the correct order.  In order to
get to all these buttons, they must also open the lower door and
deactivate the alarm tile by pushing further buttons.  To simplify
both the planning and the NLG task, the world is discretized into
tiles of equal size; the user can turn by 90 degree steps in either
direction, and can move from the center of one tile to the center of
the next.  Some actions that the user can take are shown in
Fig.~\ref{fig:give-planning}.  The first few steps of one shortest
plan to achieve the goal are:
\begin{enumerate}
\item $\mathsf{turn-left}(\mathsf{north}, \mathsf{west})$
\item $\mathsf{move}(\mathsf{pos\_5\_2}, \mathsf{pos\_4\_2}, \mathsf{west})$
\item $\mathsf{manipulate-b1-off-on}(\mathsf{pos\_5\_2})$
\item $\mathsf{turn-right}(\mathsf{west}, \mathsf{north})$
\end{enumerate}

\begin{figure}
  \centering
  xxxxx
  \caption{Excerpt from the GIVE planning problem.}
  \label{fig:give-planning}
\end{figure}

This shortest plan consists of 108 action instances, most of which are
``move'' actions.  The bulk of the GIVE problem is therefore a variant
of the Gridworld problem, which also involves finding a route through
a world with discrete positions -- but with the need to press buttons
in the right order and with somewhat more complicated room shapes.  In
this example, the NLG system could generate an instruction sequence
starting with ``turn left; walk forward; press the button; turn right;
walk forward; walk forward; turn right; walk forward; walk forward;
turn left; walk forward''.  However, this would be clumsy and boring,
and it is to be expected that frustrated users will quickly cancel an
interaction with such a system, resulting in a bad evaluation score.
Having just pressed the button, it would be much better to say ``turn
around and walk through the door''.  This means that the mapping from
action instances to natural language instructions is not trivial.
Conversely, it may also be necessary to express a single planning step
with several instructions.  Having just entered the top left room, it
may be easier for the user to understand the instruction ``walk to the
center of the room; turn right; now press the green button in front of
you'' than the instruction ``press the green button on the wall to
your right''.  To plan the referring expression ``the green button in
front of you'' at a time when the button is not yet in front of the
user, the NLG system must keep track of the hypothetical changes in
what objects are visible to the user.  Thus the acts of referring and
instructing, and the modules for planning and NLG, must be integrated
tightly.

\todo{equivalent plans; plan recognition; real-time requirements}


%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "experiences"
%%% End: 
